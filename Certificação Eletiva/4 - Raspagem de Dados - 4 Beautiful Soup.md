[[4 - Raspagem de Dados]]

**Beautiful Soup Ã© uma biblioteca Python para extrair dados de arquivos HTML e XML**. Ela Ã© de grande valia para analisar esses arquivos e fornecer maneiras mais simples de navegar, pesquisar e modificar a Ã¡rvore de anÃ¡lise, podendo economizar vÃ¡rias horas de trabalho.

Veremos como utilizar alguns dos recursos do Beautiful Soup 4, mas antes de analisar as informaÃ§Ãµes precisamos baixar o conteÃºdo da pÃ¡gina que queremos utilizar.

## Primeiros passos

Com base no que vocÃª jÃ¡ sabe sobreÂ _web scraping_, jÃ¡ pode ter uma ideia de ferramentas que podemos usar para fazer a requisiÃ§Ã£o para a pÃ¡gina e baixar seu conteÃºdo HTML.

Para nosso exemplo, usaremos a bibliotecaÂ `requests`(especificamente na versÃ£o 2.27.1) e faremos uma requisiÃ§Ã£o do tipoÂ `get`Â para uma pÃ¡gina de frases.

> bsoup_example.py

```python
import requests

url = "https://quotes.toscrape.com"
page = requests.get(url)
print(page.content)

```

Com o conteÃºdo da pÃ¡gina baixado, vamos ao que interessa, ver como utilizar o Beautiful Soup para fazer sua anÃ¡lise!

## InstalaÃ§Ã£o das bibliotecas

> âš ï¸ Lembre-se de estar em um ambiente virtual antes de fazer as instalaÃ§Ãµes de bibliotecas.

Para instalar as bibliotecasÂ `Beautiful Soup`Â eÂ `requests`Â basta digitar em seu terminal:

```bash
pip3 install beautifulsoup4==4.11.1 requests==2.27.1
```

Agora podemos importar a lib em nosso arquivo e comeÃ§ar a explorar as funcionalidades e facilidades que a ferramenta oferece.

```python
# import requests
from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
html_content = page.text

# Cria uma instÃ¢ncia do objeto Beautiful Soup e usa o parser de HTML nativo
# do Python
soup = BeautifulSoup(html_content, "html.parser")

# Utiliza o mÃ©todo prettify para melhorar a visualizaÃ§Ã£o do conteÃºdo
print(soup.prettify())

```

O retorno desse cÃ³digo Ã© o HTML da pÃ¡gina, jÃ¡ formatado de uma forma muito amigÃ¡vel para a leitura, nÃ£o concorda?

## Tipos de objetos do Beautiful Soup

**O Beautiful Soup transforma um documento HTML complexo em uma Ã¡rvore de objetos Python.**Â Os quatro tipos de objetos que podemos lidar sÃ£oÂ `Tag`,Â `NavigableString`,Â `BeautifulSoup`Â eÂ `Comment`. Na documentaÃ§Ã£o, que estÃ¡ disponÃ­vel inclusive em portuguÃªs, existe uma seÃ§Ã£o inteira dedicada aosÂ [tipos de objetos](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ptbr/#tipos-de-objetos), mas destacaremos aqui apenas os dois primeiros.

### Tag

Em suma,Â **um objeto do tipoÂ `Tag`Â corresponde a uma tagÂ `XML`Â ouÂ `HTML`Â do documento original**. TodaÂ _tag_Â possui um nome acessÃ­vel atravÃ©s deÂ `.name`. Por exemplo, quando vemosÂ `<header>`, ele Ã© um elemento do tipoÂ _tag_Â e o nome dessa tag Ã©Â `header`.

AsÂ _tags_Â tambÃ©m podem ter atributos, como classes, ids e etc. Esses atributos sÃ£o acessÃ­veis considerandoÂ _tag_Â como um dicionÃ¡rio e como podem receber mÃºltiplos valores, sÃ£o apresentados em forma de lista.

```python
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")


# acessando a tag 'title'
title = soup.title

# retorna o elemento HTML da tag
print(title)

# o tipo de 'title' Ã© tag
print(type(title))

# o nome de 'title' Ã© title
print(title.name)

# acessando a tag 'footer'
footer = soup.footer

# acessando o atributo classe da tag footer
print(footer['class'])

```

### NavigableString

Uma string corresponde a um texto dentro de umaÂ _tag_Â e esse texto fica armazenado na classeÂ `NavigableString`.

```python
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# title = soup.title
# footer = soup.footer

# retorna o elemento HTML da tag
print(title)

# Acessando a string de uma tag
print(title.string)

# Verificando o tipo dessa string
print(type(title.string))

```

## Buscando na Ã¡rvore

Assim como nas outras ferramentas apresentadas atÃ© aqui, oÂ **Beautiful Soup tambÃ©m possui dois mÃ©todos principais para encontrar elementos**. Eles sÃ£o oÂ **`find()`Â eÂ `find_all()`**Â e a essa altura vocÃª jÃ¡ deve ter presumido que a diferenÃ§a bÃ¡sica entre eles Ã© que o primeiro retorna apenas o primeiro elemento que corresponder ao filtro, enquanto o segundo retorna a lista de todos os elementos que baterem com o filtro.

HÃ¡ vÃ¡rias possibilidades de filtros a serem utilizados dentro dos mÃ©todos descritos acima, deÂ _strings_Â eÂ _regex_, atÃ© funÃ§Ãµes, eÂ [ler a documentaÃ§Ã£o](https://www.crummy.com/software/BeautifulSoup/bs4/doc.ptbr/#buscando-na-arvore)Â Ã© essencial para garantir que vocÃª estÃ¡ utilizando o mÃ©todo mais adequado para buscar os dados que deseja.

Existem algumas informaÃ§Ãµes que sÃ£o bem comuns de querermos extrair, como os valores das ocorrÃªncias de determinada tag, de um atributo ou mesmo todo o texto da pÃ¡gina.

> ğŸ‘€Â **De olho na dica:**Â Ao executar o cÃ³digo abaixo, tente executar uma impressÃ£o por vez, deixando os demaisÂ `print`s comentados enquanto isso, para ter uma melhor visualizaÃ§Ã£o dos retornos. ğŸ˜‰

```python
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# Imprime todas as ocorrÃªncias da tag "p" da pÃ¡gina ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("p"))

# Imprime o elemento com o id especificado ou "None",
# caso nenhum elemento corresponda a pesquisa
print(soup.find(id="quote"))

# Imprime todo o texto da pÃ¡gina
print(soup.get_text())

# Imprime todas as "divs" que possuam a classe "quote" ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("div", {"class": "quote"}))

```

Por debaixo dos panos,Â `soup.find_all("p")`Â eÂ `soup.find_all(name="p")`Â sÃ£o a mesma coisa, da mesma forma queÂ `soup.find(id="quote")`Â Ã© o mesmo queÂ `soup.find(attrs={"id": "quote"})`. Isso se deve ao fato de argumentos nomeados diferentes deÂ `name`,Â `attrs`,Â `recursive`,Â `string`Â eÂ `limit`Â serem todos colocados no dicionÃ¡rio dentro do parÃ¢metroÂ `attrs`.

Para dar uma visÃ£o geral do que podemos utilizar e da simplicidade de fazerÂ _scraping_Â com o Beautiful Soup, vamos fazer algo similar ao que fizemos no exemplo de Selenium e raspar as informaÃ§Ãµes de uma pÃ¡gina de notÃ­cias do site Tecmundo.

> scrape_bsoup.py

```python
import requests
from bs4 import BeautifulSoup


# Acessa uma URL e retorna um objeto do Beautiful Soup
def get_html_soup(url):
    page = requests.get(url)
    html_page = page.text

    soup = BeautifulSoup(html_page, "html.parser")
    soup.prettify()
    return soup


# Recebe um objeto soup e retorna informaÃ§Ãµes das notÃ­cias de uma pÃ¡gina
def get_page_news(soup):

    # Define uma lista vazia a ser populada com os dados do scraping
    news = []

    # Percorre todos os elementos da tag 'article' com a classe especificada
    for post in soup.find_all(
        "article", {"class": "tec--card tec--card--medium"}
    ):

        # Cria um dicionÃ¡rio para guardar os elementos a cada iteraÃ§Ã£o
        item = {}

        # Cria um item chamado tag no dicionÃ¡rio para guardar a tag do post
        # Primeiro pesquisa pela div com a classe especÃ­fica
        # Depois pela tag 'a' dentro dos resultados do primeiro filtro
        # Por fim, traz o resultado da string dentro da tag a
        item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

        # Mesma lÃ³gica da busca anterior
        item["title"] = post.find("h3", {"class": "tec--card__title"}).a.string

        # Parecido com o que foi feito anteriormente, mas dessa vez pega
        # o atributo 'href' dentro da tag 'a'
        item["link"] = post.find("h3", {"class": "tec--card__title"}).a["href"]

        # Mesma lÃ³gica da primeira busca, mas trazendo a string dentro da 'div'
        # direto
        item["date"] = post.find(
            "div", {"class": "tec--timestamp__item z--font-semibold"}
        ).string

        # Mesma lÃ³gica da busca anterior
        item["time"] = post.find(
            "div", {"class": "z--truncate z-flex-1"}
        ).string

        # Adiciona os itens criado no dicionÃ¡rio Ã  lista 'news'
        news.append(item)

    return news


print(get_page_news(get_html_soup("https://www.tecmundo.com.br/novidades")))

```

Para fazer a paginaÃ§Ã£o e extrair todas as notÃ­cias do site, a lÃ³gica Ã© bem similar a utilizada com oÂ `Parsel`Â e oÂ `Selenium`.

```python
# import requests
# from bs4 import BeautifulSoup


# # Acessa uma URL e retorna um objeto do Beautiful Soup
# def get_html_soup(url):
#     page = requests.get(url)
#     html_page = page.text

#     soup = BeautifulSoup(html_page, "html.parser")
#     soup.prettify()
#     return soup


# # Recebe um objeto soup e retorna informaÃ§Ãµes das notÃ­cias de uma pÃ¡gina
# def get_page_news(soup):

#     news = []

#     for post in soup.find_all(
#         "article", {"class": "tec--card tec--card--medium"}
#     ):

#         item = {}

#         item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

#         item["title"] = post.find("h3", {"class": "tec--card__title"}).a.string

#         item["link"] = post.find("h3", {"class": "tec--card__title"}).a["href"]

#         item["date"] = post.find(
#             "div", {"class": "tec--timestamp__item z--font-semibold"}
#         ).string

#         item["time"] = post.find(
#             "div", {"class": "z--truncate z-flex-1"}
#         ).string

#         news.append(item)

#     return news


# Recebe um objeto soup e retorna o link que redireciona
# para a pÃ¡gina seguinte, caso ele exista
def get_next_page(soup_page):
    try:

        # Busca pela tag 'a' com as classes especÃ­ficas do link desejado
        return soup_page.find(
            "a",
            {"class": "tec--btn"},
        )["href"]
    except TypeError:
        return None


# Recebe uma URL e retorna uma lista com todas as notÃ­cias do site
def scrape_all(url):
    all_news = []

    # Enquanto a pesquisa pelo link que vai para a pÃ¡gina seguinte existir
    while get_next_page(get_html_soup(url)) is not None:

        # Imprime a URL da pÃ¡gina seguinte
        print(get_next_page(get_html_soup(url)))

        # Adiciona os elementos da lista com as notÃ­cias de cada
        # pÃ¡gina na lista 'all_news'
        all_news.extend(get_page_news(get_html_soup(url)))

        # define a pÃ¡gina seguinte como URL para a prÃ³xima iteraÃ§Ã£o
        url = get_next_page(get_html_soup(url))

    return all_news


# Vamos comeÃ§ar perto das Ãºltimas pÃ¡ginas pra nÃ£o ter que fazer a requisiÃ§Ã£o
# do site inteiro
print(scrape_all("https://www.tecmundo.com.br/novidades?page=11030"))

```

As duas ferramentas que vocÃª viu hoje te deram mais possibilidades de fazer raspagem de dados e podem ampliar seu ferramental para ir muito alÃ©m na sua carreira, explorando outras funcionalidades delas, buscando como integrÃ¡-las ou atÃ© mesmo procurando outras ferramentas que tragam ainda mais simplicidade e praticidade ao que vocÃª precisa fazer.

###### Fonte: [Curse](https://app.betrybe.com/learn/course/5e938f69-6e32-43b3-9685-c936530fd326/module/3d93d491-e3ed-409f-bdb6-3a5dcd11f8d2/section/61c046bb-0372-49d7-83c1-fe68b9e01d73/day/da613349-45be-4061-a2e9-f651479a2a51/lesson/19b63fed-4017-4848-9668-efa9631d2b43)

