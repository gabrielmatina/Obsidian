[[4 - Raspagem de Dados]]

# Requisi√ß√µes web

A comunica√ß√£o com servidores HTTP e HTTPS se d√° atrav√©s de requisi√ß√µes, nas quais utilizamos o verbo para indicar que tipo de a√ß√£o deve ser tomada para um dado recurso. Devemos informar na requisi√ß√£o em qual recurso estamos atuando e no cabe√ßalho passamos algumas informa√ß√µes que podem ser importantes, como o tipo de resposta aceita ou o tipo de conte√∫do sendo enviado.

O¬†**Python**¬†possui um pacote para lidar com o protocolo HTTP o¬†`urllib`, por√©m seu uso √© mais verboso. Por isso vamos utilizar a biblioteca¬†`requests`, que possui uma interface mais amig√°vel e √© bem difundida na comunidade.

Voc√™ j√° pode instalar a¬†`requests`¬†dentro de um ambiente virtual, com os seguintes comandos:

```bash
python3 -m venv .venv && source .venv/bin/activate
python3 -m pip install requests
```

Abaixo vamos ver alguns exemplos de como utilizar a biblioteca¬†`requests`:

```python
import requests


# Requisi√ß√£o do tipo GET
response = requests.get("https://www.betrybe.com/")
print(response.status_code)  # c√≥digo de status
print(response.headers["Content-Type"])  # conte√∫do no formato html

# Conte√∫do recebido da requisi√ß√£o
print(response.text)

# Bytes recebidos como resposta
print(response.content)

# Requisi√ß√£o do tipo post
response = requests.post("http://httpbin.org/post", data="some content")
print(response.text)

# Requisi√ß√£o enviando cabe√ßalho (header)
response = requests.get("http://httpbin.org/get", headers={"Accept": "application/json"})
print(response.text)

# Requisi√ß√£o a recurso bin√°rio
response = requests.get("http://httpbin.org/image/png")
print(response.content)

# Recurso JSON
response = requests.get("http://httpbin.org/get")
# Equivalente ao json.loads(response.content)
print(response.json())

# Podemos tamb√©m pedir que a resposta lance uma exce√ß√£o caso o status n√£o seja OK
response = requests.get("http://httpbin.org/status/404")
response.raise_for_status()
```

# Alguns problemas

## Rate Limit

Muitas vezes a p√°gina de onde estamos removendo o conte√∫do possui uma limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo, evitando assim ataques de nega√ß√£o de servi√ßo.

Isto pode levar a um bloqueio por um curto per√≠odo de tempo ou at√© mesmo banimento de acessar um recurso.

Que tal vermos um exemplo? üòé

```python
import requests


# √Ä partir da d√©cima requisi√ß√£o somos bloqueados de acessar o recurso
# C√≥digo de status 429: Too Many Requests
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response.status_code)
```

Neste exemplo, ap√≥s a d√©cima requisi√ß√£o, o servidor come√ßa a nos retornar respostas com o c√≥digo de¬†`status 429`, ‚ÄúToo Many Requests‚Äù. Isto significa que estamos utilizando uma taxa de solicita√ß√£o maior que a suportada. Ele permanecer√° assim por algum tempo (1 minuto).

![denial](https://content-assets.betrybe.com/prod/a298a46a-e0bb-44ad-8ca1-90d7e287e244-denial.gif)

Uma boa pr√°tica √© sempre fazermos uma pausa entre as requisi√ß√µes, ou lote delas, mesmo que o website onde a informa√ß√£o est√° n√£o fa√ßa o bloqueio. Assim, evitamos tirar o site do ar.

```python
import requests
import time


# Coloca uma pausa de 6 segundos a cada requisi√ß√£o
# Obs: este site de exemplo tem um rate limit de 10 requisi√ß√µes por minuto
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response)
    time.sleep(6)
```

## Timeout

√Ås vezes pedimos um recurso ao servidor, mas caso o nosso tr√°fego de rede esteja lento ou exista um problema interno do servidor, nossa resposta pode demorar ou at√© mesmo ficar travada indefinidamente.

_Como garantir, ap√≥s um tempo, que o recurso seja retornado?_¬†ü§î

```python
import requests

# Por 10 segundos n√£o temos certeza se a requisi√ß√£o ir√° retornar
response = requests.get("https://httpbin.org/delay/10")
print(response)
```

Podemos definir um¬†**tempo limite (timeout)**¬†para que, ap√≥s este tempo, possamos tomar alguma atitude, como por exemplo, realizar uma nova tentativa.

Este tempo limite normalmente √© definido atrav√©s de experimenta√ß√µes e an√°lise do tempo de resposta normal de uma requisi√ß√£o.

```python
import requests


try:
    # recurso demora muito a responder
    response = requests.get("http://httpbin.org/delay/10", timeout=2)
except requests.ReadTimeout:
    # vamos fazer uma nova requisi√ß√£o
    response = requests.get("http://httpbin.org/delay/1", timeout=2)
finally:
    print(response.status_code)
```

> No exemplo acima, para efeitos did√°ticos, modificamos a URI do recurso, diminuindo o delay de resposta da requisi√ß√£o atrav√©s do¬†`timeout`, por√©m normalmente este valor n√£o muda.


# Analisando respostas

> Para realizar a extra√ß√£o de dados de um conte√∫do web vamos utilizar uma biblioteca chamada¬†`parsel`. Ela pode ser instalada com o seguinte comando abaixo:

```bash
python3 -m pip install parsel
```

**Vamos para a pr√°tica?**¬†üí™

Como j√° aprendemos a realizar requisi√ß√µes HTTP e buscar conte√∫do, vamos agora analisar este conte√∫do e extrair informa√ß√µes.

Para ajudar na did√°tica, vamos utilizar o site¬†`http://books.toscrape.com/`. Ele √© um site pr√≥prio para o treinamento de raspagem de dados.

Para come√ßar, vamos acessar o site e ver o seu conte√∫do.

![Livros √† venda](https://content-assets.betrybe.com/prod/53606aaf-b51f-404a-937c-3037924336d0-Livros%20%C3%A0%20venda.png)
Livros √† venda

Em c√≥digo, vamos baixar o conte√∫do da p√°gina e criar um seletor, que ser√° utilizado para realizarmos as extra√ß√µes. Vamos criar o arquivo¬†`.py`¬†para buscarmos as informa√ß√µes:

> exemplo_scrape.py

```python
from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
print(selector)
```

_Ok_, que tal extrairmos todos os livros desta primeira p√°gina e buscar seus t√≠tulos e pre√ßos?

Para conseguirmos extrair essas informa√ß√µes precisamos, primeiro, inspecionar cada um dos elementos, buscando algo que os identifique de forma √∫nica na p√°gina.

![Inspecionando o t√≠tulo](https://content-assets.betrybe.com/prod/53606aaf-b51f-404a-937c-3037924336d0-Inspecionando%20o%20t%C3%ADtulo.png)
Inspecionando o t√≠tulo

> exemplo_scrape.py

```python
# ...


# response = requests.get("http://books.toscrape.com/")
# selector = Selector(text=response.text)

# O t√≠tulo est√° no atributo title em um elemento √¢ncora (<a>)
# Dentro de um h3 em elementos que possuem classe product_pod
titles = selector.css(".product_pod h3 a::attr(title)").getall()
# Estamos utilizando a::attr(title) para capturar somente o valor contido no texto do seletor

# Os pre√ßos est√£o no texto de uma classe price_color
# Que se encontra dentro da classe .product_price
prices = selector.css(".product_price .price_color::text").getall()

# Combinando tudo podemos buscar os produtos
# em em seguida buscar os valores individualmente
for product in selector.css(".product_pod"):
    title = product.css("h3 a::attr(title)").get()
    price = product.css(".price_color::text").get()
    print(title, price)

```

O seletor principal que cont√©m todo o conte√∫do da p√°gina pode ser utilizado em uma busca para encontrar seletores mais espec√≠ficos. Podemos fazer isto utilizando a fun√ß√£o¬†`css`. Ela recebe um seletor CSS como par√¢metro, embora podemos passar um tipo especial de seletor quando queremos algum valor bem espec√≠fico como o texto ou um atributo.

Ap√≥s definir o seletor, podemos utilizar a fun√ß√£o¬†`get`¬†para buscar o primeiro seletor/valor que satisfa√ßa aquela busca. A fun√ß√£o¬†`getall`¬†√© similar, por√©m traz todos os valores que satisfa√ßam aquele seletor.

Assim como temos a fun√ß√£o¬†`css`¬†que faz a busca por seletores CSS, temos tamb√©m a fun√ß√£o¬†`xpath`, que faz a busca com base em XPath.

> üëÄ¬†**De olho na dica**: existem sites que podem ajudar com seletores, como¬†[css](https://devhints.io/css)¬†ou¬†[xpath](https://devhints.io/xpath).

# Recursos paginados

Tudo certo, temos 20 livros, mas sabemos que na verdade este site possui¬†**1000**¬†livros. Que tal coletarmos todos?!

![Navega√ß√£o at√© a p√°gina 2](https://content-assets.betrybe.com/prod/Navega%C3%A7%C3%A3o%20at%C3%A9%20a%20p%C3%A1gina%202.png)
Navega√ß√£o at√© a p√°gina 2

Navegando um pouco por entre as p√°ginas, percebemos que cada p√°gina possui uma refer√™ncia para a pr√≥xima. Mas, se a¬†`URL`¬†√© sequencial, por que n√£o jogamos valores at√© a p√°gina avisar que o recurso n√£o foi encontrado? ü§î

Acontece que podemos evitar fazer requisi√ß√µes desnecess√°rias, j√° que a p√°gina nos informa a pr√≥xima.

O que precisamos fazer √© criar um seletor com a p√°gina, extrair os dados, buscar a nova p√°gina e repetir todo o processo, at√© termos navegados em todas as p√°ginas.

At√© a parte da extra√ß√£o n√≥s j√° fizemos, vamos ent√£o dar uma olhada em como buscar a pr√≥xima p√°gina:

![Inspecionando o bot√£o de pr√≥ximo](https://content-assets.betrybe.com/prod/Inspecionando%20o%20bot%C3%A3o%20de%20pr%C3%B3ximo.png)
Inspecionando o bot√£o de pr√≥ximo

> exemplo_scrape.py

```python
# ...
# for product in selector.css(".product_pod"):
#     title = product.css("h3 a::attr(title)").get()
#     price = product.css(".price_color::text").get()
#     print(title, price)

# Existe uma classe next, que podemos recuperar a url atrav√©s do seu elemento √¢ncora
next_page_url = selector.css(".next a::attr(href)").get()
print(next_page_url)
```

Agora que sabemos como recuperar a pr√≥xima p√°gina, podemos iniciar na primeira p√°gina e continuar buscando livros enquanto uma nova p√°gina for encontrada. Cada vez que encontrarmos uma p√°gina, extra√≠mos seus dados e continuamos at√© acabarem as p√°ginas. Ent√£o vamos alterar tudo que t√≠nhamos escrito no arquivo¬†`exemplo_scrape.py`¬†para o c√≥digo abaixo:

> exemplo_scrape.py

```python
from parsel import Selector
import requests


# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
URL_BASE = "http://books.toscrape.com/catalogue/"
next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    response = requests.get(URL_BASE + next_page_url)
    selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        title = product.css("h3 a::attr(title)").get()
        price = product.css(".price_color::text").get()
        print(title, price)
    # Descobre qual √© a pr√≥xima p√°gina
    next_page_url = selector.css(".next a::attr(href)").get()
```

Nossa, quantos livros! üìö

# Recursos obtidos √† partir de outro recurso

_Agora que estamos coletando todos os livros, que tal coletarmos tamb√©m a descri√ß√£o de cada livro?_

O problema √© que a descri√ß√£o s√≥ pode ser acessada navegando at√© a p√°gina espec√≠fica de cada livro.

![Descri√ß√£o do livro est√° em sua p√°gina de detalhes](https://content-assets.betrybe.com/prod/Descri%C3%A7%C3%A3o%20do%20livro%20est%C3%A1%20em%20sua%20p%C3%A1gina%20de%20detalhes.png)Descri√ß√£o do livro est√° em sua p√°gina de detalhes

‚ñ∂Ô∏è O primeiro passo √© investigarmos como descobrir a¬†`URL`¬†que nos leva at√© a p√°gina de detalhes de um produto. üîç

![Inspecionar da descri√ß√£o do livro est√° em sua p√°gina de detalhes](https://content-assets.betrybe.com/prod/Inspecionar%20da%20descri%C3%A7%C3%A3o%20do%20livro%20est%C3%A1%20em%20sua%20p%C3%A1gina%20de%20detalhes.png)
Inspecionar da descri√ß√£o do livro est√° em sua p√°gina de detalhes

Com esse seletor em m√£os, precisamos recuperar o atributo¬†`href`¬†que cont√©m o link para a p√°gina de detalhes do livro. Vamos criar um outro arquivo, apenas para fazer o teste da¬†_feature_¬†que queremos implementar, depois vamos alterar o c√≥digo do arquivo¬†`exemplo_scrape.py`¬†para realmente implementar a fun√ß√£o. ‚ö†Ô∏èLembre-se de criar o arquivo no mesmo diret√≥rio que j√° estava utilizando.

> teste.py

```python
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

# Vamos testar com a primeira p√°gina
response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

# Recupera o atributo href do primeiro elemento que combine com o seletor
href = selector.css(".product_pod h3 a::attr(href)").get()
print(href)

# Para obter a url completa, basta adicionar a nossa URL_BASE
print(URL_BASE + href)
```

‚ñ∂Ô∏è Em seguida, acessamos a p√°gina de detalhes do produto, e inspecionamos a descri√ß√£o do produto.

![Inspecionando a descri√ß√£o de um livro](https://content-assets.betrybe.com/prod/Inspecionando%20a%20descri%C3%A7%C3%A3o%20de%20um%20livro.png)
Inspecionando a descri√ß√£o de um livro.

‚ñ∂Ô∏è A descri√ß√£o do produto est√° uma tag¬†`p`, ‚Äúirm√£‚Äù de uma tag com id¬†`product_description`. Isto pode ser expresso atrav√©s do seletor¬†`a`.

No c√≥digo, precisamos realizar uma nova requisi√ß√£o √† p√°gina de detalhes e depois analisaremos seu conte√∫do em busca da descri√ß√£o do produto. Para isso, vamos alterar todo o conte√∫do novamente, por√©m dessa vez ser√° o conte√∫do do arquivo¬†`teste.py`:

> teste.py

```python
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

href = selector.css(".product_pod h3 a::attr(href)").get()
detail_page_url = URL_BASE + href

# baixa o conte√∫do da p√°gina de detalhes
detail_response = requests.get(detail_page_url)
detail_selector = Selector(text=detail_response.text)

# recupera a descri√ß√£o do produto
# ~ significa a tag irm√£
description = detail_selector.css("#product_description ~ p::text").get()
print(description)
```

‚ñ∂Ô∏è Por fim, vamos adicionar a l√≥gica para buscar a descri√ß√£o do produto no nosso c√≥digo existente:

> exemplo_scrape.py

```python
# from parsel import Selector
# import requests


# URL_BASE = "http://books.toscrape.com/catalogue/"
# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
# next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    # response = requests.get(URL_BASE + next_page_url)
    # selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        # Busca e extrai o t√≠tulo e  o pre√ßo
        # title = product.css("h3 a::attr(title)").get()
        # price = product.css(".price_color::text").get()
        # print(title, price)

        # Busca o detalhe de um produto
        detail_href = product.css("h3 a::attr(href)").get()
        detail_page_url = URL_BASE + detail_href

        # Baixa o conte√∫do da p√°gina de detalhes
        detail_response = requests.get(detail_page_url)
        detail_selector = Selector(text=detail_response.text)

        # Extrai a descri√ß√£o do produto
        description = detail_selector.css("#product_description ~ p::text").get()
        print(description)

    # Descobre qual √© a pr√≥xima p√°gina
    # next_page_url = selector.css(".next a::attr(href)").get()
```

###### Fonte: [Curse](https://app.betrybe.com/learn/course/5e938f69-6e32-43b3-9685-c936530fd326/module/3d93d491-e3ed-409f-bdb6-3a5dcd11f8d2/section/61c046bb-0372-49d7-83c1-fe68b9e01d73/day/29e6c74d-baff-46b1-9880-3ce81ab1a33e/lesson/965e5c50-d350-4a94-ae3a-8bcc6af19756)

